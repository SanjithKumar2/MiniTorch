{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "from MiniTorch.core.baseclasses import ComputationNode\n",
    "from MiniTorch.legacy_utils import _conv2d_forward_legacy_v1, _conv2d_forward_legacy_v2, _conv2d_backward_legacy_v1, _conv_initialize_legacy, get_kernel_size, get_stride, _conv2d_backward_legacy_v2\n",
    "import time\n",
    "from typing import Literal, List, Tuple, Dict, Any\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(ComputationNode):\n",
    "\n",
    "    def __init__(self, input_channels : int,kernel_size : int | tuple = 3, no_of_filters = 1, stride = 1, pad = 0, accumulate_grad_norm = False, accumulate_params = False,seed_key = None, bias = True, \n",
    "                 initialization = \"None\", use_legacy_v1 : bool = False, use_legacy_v2:bool = False):\n",
    "        super().__init__()\n",
    "        if seed_key == None:\n",
    "            self.seed_key = jrandom.PRNGKey(int(time.time()))\n",
    "        self.kernel_size = get_kernel_size(kernel_size)\n",
    "        self.input_channels = input_channels\n",
    "        self.no_of_filters = no_of_filters\n",
    "        self.stride = get_stride(stride)\n",
    "        self.pad = pad\n",
    "        self.accumulate_grad_norm = accumulate_grad_norm\n",
    "        self.accumulate_params = accumulate_params\n",
    "        self.initialization = initialization\n",
    "        self.parameters = {'W': None, 'b': None}\n",
    "        self.bias = bias\n",
    "\n",
    "        self.use_legacy_v1 = use_legacy_v1\n",
    "        self.use_legacy_v2 = use_legacy_v2\n",
    "        if use_legacy_v1 or use_legacy_v2:\n",
    "            self.parameters['W'], self.parameters['b'] = _conv_initialize_legacy(self.kernel_size,self.no_of_filters,self.input_channels,self.initialization,self.bias)\n",
    "        else:\n",
    "            self.initialize(self.seed_key)\n",
    "    def initialize(self, seed_key):\n",
    "        if self.initialization == \"he\":\n",
    "            self.parameters['W'] = jrandom.normal(seed_key, (self.no_of_filters, self.input_channels, self.kernel_size[0], self.kernel_size[1])) * jnp.sqrt(2/(self.no_of_filters * self.kernel_size[0] * self.kernel_size[1]))\n",
    "        else:\n",
    "            self.parameters['W'] = jrandom.normal(seed_key, (self.no_of_filters, self.input_channels, self.kernel_size[0], self.kernel_size[1]))\n",
    "        if self.bias:\n",
    "            self.parameters['b'] = jnp.zeros((self.no_of_filters,))\n",
    "\n",
    "    @staticmethod\n",
    "    def _conv2d_forward(X : jax.Array, W : jax.Array,b : jax.Array, stride : tuple, padding: Literal['VALID','SAME'] = 'VALID'):\n",
    "\n",
    "        def conv_over_one_batch(X_vec, W_vec, stride, padding):\n",
    "\n",
    "            if X_vec.ndim == 3:\n",
    "                X_vec = X_vec[None,...]\n",
    "            cvout = jax.lax.conv_general_dilated(X_vec,W_vec[None,...],window_strides=stride,padding=padding,\n",
    "                                                    dimension_numbers=('NCHW','OIHW','NCHW'))[0,0]\n",
    "            return cvout\n",
    "        convout = jax.vmap(jax.vmap(conv_over_one_batch,in_axes=(None,0,None,None)), in_axes=(0,None,None,None))(X,W,stride,padding)\n",
    "        convout += b\n",
    "        return convout\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        if self.use_legacy_v1:\n",
    "            x = np.pad(x,((0,0),(0,0),(self.pad,self.pad),(self.pad,self.pad)))\n",
    "            self.output = _conv2d_forward_legacy_v1(self.parameters['W'], x, self.stride, self.parameters['b'])\n",
    "            return self.output\n",
    "        if self.use_legacy_v2:\n",
    "            x = np.pad(x,((0,0),(0,0),(self.pad,self.pad),(self.pad,self.pad)))\n",
    "            self.output = _conv2d_forward_legacy_v2(self.parameters['W'], x, self.stride, self.parameters['b'])\n",
    "            return self.output\n",
    "        W, b, stride = self.parameters['W'], self.parameters['b'], self.stride\n",
    "        with jax.checking_leaks():\n",
    "            output = jax.jit(Conv2D._conv2d_forward, static_argnames=('stride','padding'))(x, W, b, stride)\n",
    "        self.output = output\n",
    "        return self.output\n",
    "    def backward(self, out_grad):\n",
    "        dL_dW,dL_db,dL_dinput = None,None,None\n",
    "        if self.use_legacy_v1:\n",
    "            dL_dW,dL_db,dL_dinput = _conv2d_backward_legacy_v1(out_grad,self.input,self.kernel_size,self.parameters['W'],self.parameters['b'],self.stride,self.pad)\n",
    "        if self.use_legacy_v2:\n",
    "            dL_dW,dL_db,dL_dinput = _conv2d_backward_legacy_v2(out_grad,self.input,self.kernel_size,self.parameters['W'],self.parameters['b'],self.stride,self.pad)\n",
    "        self.grad_cache['dL_dW'] = dL_dW\n",
    "        self.grad_cache['dL_db'] = dL_db\n",
    "        self.grad_cache['dL_dinput'] = dL_dinput\n",
    "        return dL_dinput\n",
    "    \n",
    "    def weights_var_mean(self):\n",
    "        return self.parameters['W'].var(), self.parameters['W'].mean()\n",
    "\n",
    "    def bias_var_mean(self):\n",
    "        return self.parameters['b'].var(), self.parameters['b'].mean()\n",
    "\n",
    "    def step(self, lr):\n",
    "        if self.accumulate_grad_norm:\n",
    "            self._accumulate_grad_norm('dL_dW')\n",
    "            self._accumulate_grad_norm('dL_db')\n",
    "        if self.accumulate_params:\n",
    "            self._accumulate_parameters('W', self.weights_var_mean)\n",
    "            self._accumulate_parameters('b', self.bias_var_mean)\n",
    "        self.parameters['W'] -= lr * self.grad_cache['dL_dW']\n",
    "        if self.bias:\n",
    "            self.parameters['b'] -= lr * self.grad_cache['dL_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(ComputationNode):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.requires_grad = False\n",
    "        self.shape = None\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.shape = x.shape\n",
    "        self.input = x\n",
    "        self.output = np.reshape(x,(x.shape[0],-1))\n",
    "        return self.output\n",
    "    def backward(self, output_grad):\n",
    "        dL_dinput= np.reshape(output_grad,(self.shape[0],self.shape[1],self.shape[2],self.shape[3]))\n",
    "        self.grad_cache['dL_dinput']  = dL_dinput\n",
    "        return dL_dinput\n",
    "    \n",
    "class MaxPool2d(ComputationNode):\n",
    "    def __init__(self, pool_size, pool_stride, use_legacy_v1 = False):\n",
    "        super().__init__()\n",
    "        self.pool_size = get_kernel_size(pool_size)\n",
    "        self.stride = get_stride(pool_stride)\n",
    "        self.use_legacy_v1 = use_legacy_v1\n",
    "    @staticmethod\n",
    "    def _maxpool2d_forward_legacy_v1(pool_size, stride, input):\n",
    "        batch_size, input_channels, H, W = input.shape[0],input.shape[1], input.shape[2], input.shape[3]\n",
    "        output_h = (H - pool_size[0])//stride[0] + 1\n",
    "        output_w = (W - pool_size[1])//stride[1] + 1\n",
    "        output = np.zeros((batch_size,input_channels,output_h,output_w))\n",
    "        for b in range(batch_size):\n",
    "            for c in range(input_channels):\n",
    "                for i in range(output_h):\n",
    "                    for j in range(output_w):\n",
    "                        h_s = i * stride[0]\n",
    "                        h_e = h_s + pool_size[0]\n",
    "                        w_s = j * stride[1]\n",
    "                        w_e = w_s + pool_size[1]\n",
    "                        output[b,c,i,j] = np.max(input[b,c,h_s:h_e,w_s:w_e])\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def _maxpool2d_backward_legacy_v1(pool_size, input, out_grad, stride):\n",
    "        batch_size, input_channels = input.shape[0],input.shape[1]\n",
    "        out_grad_h, out_grad_w = out_grad.shape[2], out_grad.shape[3]\n",
    "        dL_dinput = np.zeros_like(input)\n",
    "        for b in range(batch_size):\n",
    "            for c in range(input_channels):\n",
    "                for i in range(out_grad_h):\n",
    "                    for j in range(out_grad_w):\n",
    "                        h_s = i * stride[0]\n",
    "                        h_e = h_s + pool_size[0]\n",
    "                        w_s = j * stride[1]\n",
    "                        w_e = w_s + pool_size[1]\n",
    "                        window = input[b,c,h_s:h_e,w_s:w_e]\n",
    "                        max_ids = np.unravel_index(np.argmax(window),window.shape)\n",
    "                        dL_dinput[b,c,h_s + max_ids[0],w_s + max_ids[1]] = out_grad[b,c,i,j]\n",
    "        return dL_dinput\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.input = x\n",
    "        output = None\n",
    "        if self.use_legacy_v1:\n",
    "            output = self._maxpool2d_forward_legacy_v1(self.pool_size,self.stride,x)\n",
    "        self.output = output\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        dL_dinput = None\n",
    "        if self.use_legacy_v1:\n",
    "            dL_dinput = self._maxpool2d_backward_legacy_v1(self.pool_size,self.input,output_grad,self.stride)\n",
    "        self.grad_cache['dL_input'] = dL_dinput\n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2,3,30,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv2D(input_channels=3,kernel_size = 3, no_of_filters = 50, stride = 1, pad = 0, accumulate_grad_norm = False, accumulate_params = False,seed_key = None, bias = True, initialization = \"None\",use_legacy_v2=True)\n",
    "max_pool = MaxPool2d(2,1,True)\n",
    "flatten = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36423730850219727"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "out = conv.forward(x)\n",
    "out = max_pool.forward(out)\n",
    "out = flatten.forward(out)\n",
    "et = time.time()\n",
    "et-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = np.random.randn(*list(out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.use_legacy_v1 = False\n",
    "conv.use_legacy_v2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49361562728881836"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "grad1 = flatten.backward(grad)\n",
    "grad2 = max_pool.backward(grad1)\n",
    "in_grad1= conv.backward(grad2)\n",
    "et = time.time()\n",
    "et-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.step(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.random.randn(3,3,3)\n",
    "y = np.random.randn(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833188056945801"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "grad3 = flatten.backward(grad)\n",
    "grad4 = max_pool.backward(grad3)\n",
    "in_grad2= conv.backward(grad4)\n",
    "et = time.time()\n",
    "et-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv2d_backward_legacy_v1(out_grad: np.ndarray, input: np.ndarray, \n",
    "                             kernel_size: Tuple[int], W: np.ndarray, \n",
    "                             b: np.ndarray, stride: Tuple[int], \n",
    "                             pad: int) -> np.ndarray:\n",
    "    batch_size, out_channel, out_h, out_w = out_grad.shape\n",
    "    in_channel = input.shape[1]\n",
    "    \n",
    "    # Initialize gradients\n",
    "    dL_dinput = np.zeros_like(input)\n",
    "    dL_dW = np.zeros_like(W)\n",
    "    dL_db = np.zeros_like(b)\n",
    "    \n",
    "    # Pad input gradient if needed\n",
    "    if pad > 0:\n",
    "        dL_dinput_padded = np.zeros((batch_size, in_channel, \n",
    "                                   input.shape[2] + 2*pad, \n",
    "                                   input.shape[3] + 2*pad))\n",
    "    else:\n",
    "        dL_dinput_padded = dL_dinput\n",
    "\n",
    "    # Compute bias gradient (vectorized over batch and spatial dimensions)\n",
    "    dL_db = np.sum(out_grad, axis=(0, 2, 3))\n",
    "\n",
    "    # Create views for vectorized operations\n",
    "    kh, kw = kernel_size\n",
    "    stride_h, stride_w = stride\n",
    "\n",
    "    # Generate index arrays for all positions at once\n",
    "    h_starts = np.arange(out_h) * stride_h\n",
    "    w_starts = np.arange(out_w) * stride_w\n",
    "    h_ends = h_starts + kh\n",
    "    w_ends = w_starts + kw\n",
    "    for c in range(out_channel):\n",
    "        patches = np.lib.stride_tricks.as_strided(\n",
    "            input,\n",
    "            shape=(batch_size, out_h, out_w, in_channel, kh, kw),\n",
    "            strides=(input.strides[0], \n",
    "                    stride_h * input.strides[2],\n",
    "                    stride_w * input.strides[3],\n",
    "                    input.strides[1], \n",
    "                    input.strides[2], \n",
    "                    input.strides[3])\n",
    "        )\n",
    "        dL_dW[c] = np.tensordot(out_grad[:, c], patches, axes=([0,1,2], [0,1,2]))\n",
    "\n",
    "        grad_reshaped = out_grad[:, c, :, :, np.newaxis, np.newaxis]\n",
    "        w_reshaped = W[c, :, :, :, np.newaxis, np.newaxis]\n",
    "        temp = np.zeros_like(dL_dinput_padded)\n",
    "        idx_h = np.arange(kh)[np.newaxis, np.newaxis, :, np.newaxis]\n",
    "        idx_w = np.arange(kw)[np.newaxis, np.newaxis, np.newaxis, :]\n",
    "        h_pos = h_starts[:, np.newaxis, np.newaxis] + idx_h\n",
    "        w_pos = w_starts[np.newaxis, :, np.newaxis] + idx_w\n",
    "        np.add.at(temp, \n",
    "                 (slice(None), slice(None), h_pos, w_pos),\n",
    "                 grad_reshaped * w_reshaped)\n",
    "\n",
    "        dL_dinput_padded += temp\n",
    "\n",
    "    if pad > 0:\n",
    "        dL_dinput = dL_dinput_padded[:, :, pad:-pad, pad:-pad]\n",
    "    else:\n",
    "        dL_dinput = dL_dinput_padded\n",
    "\n",
    "    return dL_dW, dL_db, dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = np.random.randn(*list(out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, out_channel, out_h, out_w = grad.shape\n",
    "in_channel = x.shape[1]\n",
    "kernel_size = (3,3)\n",
    "stride = (1,1)\n",
    "W, b = conv.parameters['W'], conv.parameters['b']\n",
    "# Initialize gradients\n",
    "dL_dinput = np.zeros_like(x)\n",
    "dL_dW = np.zeros_like(W)\n",
    "dL_db = np.zeros_like(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_db = np.sum(grad, axis=(0, 2, 3))\n",
    "kh, kw = kernel_size\n",
    "stride_h, stride_w = stride\n",
    "h_starts = np.arange(out_h) * stride_h\n",
    "w_starts = np.arange(out_w) * stride_w\n",
    "h_ends = h_starts + kh\n",
    "w_ends = w_starts + kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "strides = (\n",
    "x.strides[0],\n",
    "x.strides[1],\n",
    "x.strides[2] * stride_h,\n",
    "x.strides[3] * stride_w,\n",
    "x.strides[2],\n",
    "x.strides[3]\n",
    ")\n",
    "shapes = (batch_size,in_channel,out_h,out_w,kh,kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_strided = np.lib.stride_tricks.as_strided(\n",
    "    x,\n",
    "    shape = shapes,\n",
    "    strides= strides\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50, 28, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('bhwikl,bchw->ckl',pathces,grad).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 28, 28)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad[:,0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 30, 30, 3, 3, 3)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = conv.parameters['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50, 28, 28)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_reshaped = grad[:, 0, :, :, np.newaxis, np.newaxis][:,None,...]\n",
    "w_reshaped = w[0, None,:,None,None, :, :]\n",
    "temp = np.zeros_like(x)\n",
    "idx_h = np.arange(kh)[np.newaxis, np.newaxis, :, np.newaxis]\n",
    "idx_w = np.arange(kw)[np.newaxis, np.newaxis, np.newaxis, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_h = np.arange(kh)[np.newaxis, :, np.newaxis]  # (1, 3, 1)\n",
    "idx_w = np.arange(kw)[np.newaxis, np.newaxis, :]  # (1, 1, 3)\n",
    "h_pos = h_starts[:, np.newaxis, np.newaxis] + idx_h  # (28, 1, 3, 1)\n",
    "w_pos = w_starts[np.newaxis, :, np.newaxis] + idx_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]] [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "h_pos = h_starts[:, np.newaxis, np.newaxis] + idx_h\n",
    "w_pos = w_starts[np.newaxis, :, np.newaxis] + idx_w\n",
    "print(h_pos[0][1],w_pos[0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3, 1, 1, 3, 3), (2, 1, 28, 28, 1, 1))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_reshaped.shape, grad_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing arrays could not be broadcast together with shapes (28,3,1) (1,28,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgrad_reshaped\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_reshaped\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: shape mismatch: indexing arrays could not be broadcast together with shapes (28,3,1) (1,28,3) "
     ]
    }
   ],
   "source": [
    "np.add.at(temp, \n",
    "            (slice(None), slice(None), h_pos, w_pos),\n",
    "            grad_reshaped * w_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_rotated = np.rot90(w,2,axes=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 28, 28)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('bohw,oikl->bihw', grad, W_rotated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 30, 30)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.array([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 3],\n",
       "       [2, 1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.rot90(kernel,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = np.pad(grad,((0,0),(0,0),(1,1),(1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 30, 30)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('bohw,oikl->bihw',grad,w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 30, 30)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformers-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
