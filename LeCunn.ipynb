{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from MiniTorch.core.baseclasses import ComputationNode\n",
    "from MiniTorch.legacy_utils import _conv2d_forward_legacy_v1, _conv2d_forward_legacy_v2, _conv2d_backward_legacy_v1, _conv_initialize_legacy, get_kernel_size, get_stride, _conv2d_backward_legacy_v2\n",
    "import time\n",
    "from typing import Literal, List, Tuple, Dict, Any\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        5\n",
       "1        0\n",
       "2        4\n",
       "3        1\n",
       "4        9\n",
       "        ..\n",
       "69995    2\n",
       "69996    3\n",
       "69997    4\n",
       "69998    5\n",
       "69999    6\n",
       "Name: class, Length: 70000, dtype: category\n",
       "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(ComputationNode):\n",
    "\n",
    "    def __init__(self, input_channels : int,kernel_size : int | tuple = 3, no_of_filters = 1, stride = 1, pad = 0, accumulate_grad_norm = False, accumulate_params = False,seed_key = None, bias = True, \n",
    "                 initialization = \"None\", use_legacy_v1 : bool = False, use_legacy_v2:bool = False):\n",
    "        super().__init__()\n",
    "        if seed_key == None:\n",
    "            self.seed_key = jrandom.PRNGKey(int(time.time()))\n",
    "        self.kernel_size = get_kernel_size(kernel_size)\n",
    "        self.input_channels = input_channels\n",
    "        self.no_of_filters = no_of_filters\n",
    "        self.stride = get_stride(stride)\n",
    "        self.pad = pad\n",
    "        self.accumulate_grad_norm = accumulate_grad_norm\n",
    "        self.accumulate_params = accumulate_params\n",
    "        self.initialization = initialization\n",
    "        self.parameters = {'W': None, 'b': None}\n",
    "        self.bias = bias\n",
    "\n",
    "        self.use_legacy_v1 = use_legacy_v1\n",
    "        self.use_legacy_v2 = use_legacy_v2\n",
    "        if use_legacy_v1 or use_legacy_v2:\n",
    "            self.parameters['W'], self.parameters['b'] = _conv_initialize_legacy(self.kernel_size,self.no_of_filters,self.input_channels,self.initialization,self.bias)\n",
    "        else:\n",
    "            self.initialize(self.seed_key)\n",
    "    def initialize(self, seed_key):\n",
    "        if self.initialization == \"he\":\n",
    "            self.parameters['W'] = jrandom.normal(seed_key, (self.no_of_filters, self.input_channels, self.kernel_size[0], self.kernel_size[1])) * jnp.sqrt(2/(self.no_of_filters * self.kernel_size[0] * self.kernel_size[1]))\n",
    "        else:\n",
    "            self.parameters['W'] = jrandom.normal(seed_key, (self.no_of_filters, self.input_channels, self.kernel_size[0], self.kernel_size[1]))\n",
    "        if self.bias:\n",
    "            self.parameters['b'] = jnp.zeros((self.no_of_filters,))\n",
    "\n",
    "    @staticmethod\n",
    "    def _conv2d_forward(X : jax.Array, W : jax.Array,b :jax.Array, stride : tuple, padding: Literal['VALID','SAME'] = 'VALID'):\n",
    "\n",
    "        # def conv_over_one_batch(X_vec, W_vec, stride, padding):\n",
    "\n",
    "        #     if X_vec.ndim == 3:\n",
    "        #         X_vec = X_vec[None,...]\n",
    "        #     cvout = jax.lax.conv_general_dilated(X_vec,W_vec[None,...],window_strides=stride,padding=padding,\n",
    "        #                                             dimension_numbers=('NCHW','OIHW','NCHW'))[0,0]\n",
    "        #     return cvout\n",
    "        # convout = jax.vmap(jax.vmap(conv_over_one_batch,in_axes=(None,0,None,None)), in_axes=(0,None,None,None))(X,W,stride,padding)\n",
    "        convout = jax.lax.conv_general_dilated(\n",
    "        lhs=X, \n",
    "        rhs=W, \n",
    "        window_strides=stride, \n",
    "        padding=padding, \n",
    "        dimension_numbers=('NCHW', 'OIHW', 'NCHW')\n",
    "        )\n",
    "        convout += b[None,:,None,None]\n",
    "        return convout\n",
    "    @staticmethod\n",
    "    def _conv2d_backward(X : jax.Array, W : jax.Array, stride : tuple, padding: int, out_grad : jax.Array):\n",
    "        dL_db = jnp.sum(out_grad, axis=(0,2,3))\n",
    "        in_channel = X.shape[1]\n",
    "        batch_size, out_channels, out_h, out_w = out_grad.shape\n",
    "        kh, kw = W.shape[2], W.shape[3]\n",
    "        dL_dinput = jnp.zeros_like(X)\n",
    "        dL_dW = jnp.zeros_like(X)\n",
    "\n",
    "        input_strided = jax.lax.conv_general_dilated_patches(\n",
    "            X,\n",
    "            (kh, kw),\n",
    "            stride,\n",
    "            padding='VALID',\n",
    "            dimension_numbers=('NCHW','OIHW','NCHW')\n",
    "        )\n",
    "        input_strided = input_strided.reshape(batch_size,out_h,out_w,in_channel,kh,kw)\n",
    "        input_strided = input_strided.reshape(batch_size, out_h, out_w, in_channel, kh, kw)\n",
    "        dL_dW = jnp.einsum('bhwikl,bchw->cikl', input_strided, out_grad, optimize=True)\n",
    "\n",
    "        out_grad_up = jnp.zeros((batch_size, out_channels, out_h * stride[0], out_w * stride[1]))\n",
    "        out_grad_up = out_grad_up.at[:, :, ::stride[0], ::stride[1]].set(out_grad)\n",
    "        out_grad_padded = jnp.pad(out_grad_up, ((0, 0), (0, 0), (padding + 1, padding + 1), (padding + 1, padding + 1)))\n",
    "        W_rotated = jnp.rot90(W, 2, axes=(2, 3))\n",
    "        dL_dinput = jnp.einsum('bohw,oikl->bihw', out_grad_padded, W_rotated, optimize=True)\n",
    "\n",
    "        return dL_dW, dL_db, dL_dinput\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        if self.use_legacy_v1:\n",
    "            x = np.pad(x,((0,0),(0,0),(self.pad,self.pad),(self.pad,self.pad)))\n",
    "            self.output = _conv2d_forward_legacy_v1(self.parameters['W'], x, self.stride, self.parameters['b'])\n",
    "            return self.output\n",
    "        if self.use_legacy_v2:\n",
    "            x = np.pad(x,((0,0),(0,0),(self.pad,self.pad),(self.pad,self.pad)))\n",
    "            self.output = _conv2d_forward_legacy_v2(self.parameters['W'], x, self.stride, self.parameters['b'])\n",
    "            return self.output\n",
    "        W, b, stride = self.parameters['W'], self.parameters['b'], self.stride\n",
    "        with jax.checking_leaks():\n",
    "            output = jax.jit(Conv2D._conv2d_forward, static_argnames=('stride','padding'))(x, W,b, stride)            \n",
    "        self.output = output\n",
    "        return self.output\n",
    "    def backward(self, out_grad):\n",
    "        dL_dW,dL_db,dL_dinput = None,None,None\n",
    "        if self.use_legacy_v1:\n",
    "            dL_dW,dL_db,dL_dinput = _conv2d_backward_legacy_v1(out_grad,self.input,self.kernel_size,self.parameters['W'],self.parameters['b'],self.stride,self.pad)\n",
    "        elif self.use_legacy_v2:\n",
    "            dL_dW,dL_db,dL_dinput = _conv2d_backward_legacy_v2(out_grad,self.input,self.kernel_size,self.parameters['W'],self.parameters['b'],self.stride,self.pad)\n",
    "        else:\n",
    "            input, W, b, stride,pad =self.input, self.parameters['W'], self.parameters['b'], self.stride, self.pad\n",
    "            if self.pad:\n",
    "                input = jnp.pad(self.input,((0,0),(0,0),(self.pad,self.pad),(self.pad,self.pad)))\n",
    "            dL_dW,dL_db,dL_dinput = jax.jit(Conv2D._conv2d_backward,static_argnames=('stride','padding'))(input, W, stride, self.pad, out_grad)\n",
    "            if self.pad:\n",
    "                dL_dinput = dL_dinput[:,:,pad:-pad,pad:-pad]\n",
    "\n",
    "        self.grad_cache['dL_dW'] = dL_dW\n",
    "        self.grad_cache['dL_db'] = dL_db\n",
    "        self.grad_cache['dL_dinput'] = dL_dinput\n",
    "        return dL_dinput\n",
    "    \n",
    "    def weights_var_mean(self):\n",
    "        return self.parameters['W'].var(), self.parameters['W'].mean()\n",
    "\n",
    "    def bias_var_mean(self):\n",
    "        return self.parameters['b'].var(), self.parameters['b'].mean()\n",
    "\n",
    "    def step(self, lr):\n",
    "        if self.accumulate_grad_norm:\n",
    "            self._accumulate_grad_norm('dL_dW')\n",
    "            self._accumulate_grad_norm('dL_db')\n",
    "        if self.accumulate_params:\n",
    "            self._accumulate_parameters('W', self.weights_var_mean)\n",
    "            self._accumulate_parameters('b', self.bias_var_mean)\n",
    "        self.parameters['W'] -= lr * self.grad_cache['dL_dW']\n",
    "        if self.bias:\n",
    "            self.parameters['b'] -= lr * self.grad_cache['dL_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(ComputationNode):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.requires_grad = False\n",
    "        self.shape = None\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.shape = x.shape\n",
    "        self.input = x\n",
    "        self.output = jnp.reshape(x,(x.shape[0],-1))\n",
    "        return self.output\n",
    "    def backward(self, output_grad):\n",
    "        dL_dinput= jnp.reshape(output_grad,(self.shape[0],self.shape[1],self.shape[2],self.shape[3]))\n",
    "        self.grad_cache['dL_dinput']  = dL_dinput\n",
    "        return dL_dinput\n",
    "    \n",
    "class MaxPool2d(ComputationNode):\n",
    "    def __init__(self, pool_size, pool_stride, use_legacy_v1 = False):\n",
    "        super().__init__()\n",
    "        self.pool_size = get_kernel_size(pool_size)\n",
    "        self.stride = get_stride(pool_stride)\n",
    "        self.use_legacy_v1 = use_legacy_v1\n",
    "        self.max_indices = None\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(0, 1))\n",
    "    def _maxpool2d_forward(pool_size, stride, input):\n",
    "        batch_size, in_channels, in_h, in_w = input.shape\n",
    "        kh, kw = pool_size\n",
    "        stride_h, stride_w = stride\n",
    "        out_h = (in_h - kh) // stride_h + 1\n",
    "        out_w = (in_w - kw) // stride_w + 1\n",
    "\n",
    "        input_strided = jax.lax.conv_general_dilated_patches(\n",
    "            input,\n",
    "            filter_shape=(kh, kw),\n",
    "            window_strides=(stride_h, stride_w),\n",
    "            padding=((0, 0), (0, 0)),\n",
    "            dimension_numbers=('NCHW', 'OIHW', 'NCHW')\n",
    "        )  # (batch_size, in_channels, out_h, out_w, kh, kw)\n",
    "        input_strided = input_strided.reshape(batch_size, in_channels, out_h, out_w, kh, kw)\n",
    "        # Compute max and argmax over window dims (kh, kw)\n",
    "        output = jnp.max(input_strided, axis=(4, 5))  # (batch_size, in_channels, out_h, out_w)\n",
    "        max_indices = jnp.argmax(input_strided.reshape(*input_strided.shape[:4], -1), axis=-1)\n",
    "        # (batch_size, in_channels, out_h, out_w) - flat indices in kh*kw\n",
    "\n",
    "        return output, max_indices\n",
    "\n",
    "    @staticmethod\n",
    "    @partial(jax.jit, static_argnums=(0, 1))\n",
    "    def _maxpool2d_backward(pool_size, stride, input, out_grad, max_indices):\n",
    "        batch_size, in_channels, in_h, in_w = input.shape\n",
    "        kh, kw = pool_size\n",
    "        stride_h, stride_w = stride\n",
    "        out_h, out_w = out_grad.shape[2], out_grad.shape[3]\n",
    "\n",
    "        # Convert flat max_indices to 2D offsets within each window\n",
    "        max_h_offsets = max_indices // kw  # finds the row offset, wehn you flatten in the forward pass the last two dimensions (kh,kw), the max indices range from (0,kw*kh-1), and you divide by kw to ge the row\n",
    "        max_w_offsets = max_indices % kw   # finds the column offset of each index within a kernel, basically modulo by kw gives the column index within the kernel, like if max_idx  = 5 and k_w = 3 then the row_idx = 5//3 = 1 and col_idx 5%3 = 2\n",
    "\n",
    "        # Compute input positions where max occurred\n",
    "        h_starts = jnp.arange(out_h) * stride_h\n",
    "        w_starts = jnp.arange(out_w) * stride_w\n",
    "        h_pos = h_starts[None, None, :, None] + max_h_offsets[..., None]  # (b, c, h, w, 1)\n",
    "        w_pos = w_starts[None, None, :, None] + max_w_offsets[..., None]  # (b, c, h, w, 1)\n",
    "\n",
    "        # Flatten positions for scattering\n",
    "        h_pos = h_pos.reshape(-1)\n",
    "        w_pos = w_pos.reshape(-1)\n",
    "        batch_idx = jnp.repeat(jnp.arange(batch_size), in_channels * out_h * out_w)\n",
    "        chan_idx = jnp.tile(jnp.repeat(jnp.arange(in_channels), out_h * out_w), batch_size)\n",
    "        out_grad_flat = out_grad.reshape(-1)\n",
    "\n",
    "        # Scatter gradients to dL_dinput\n",
    "        dL_dinput = jnp.zeros_like(input)\n",
    "        indices = (batch_idx, chan_idx, h_pos, w_pos)\n",
    "        print(indices)\n",
    "        dL_dinput = dL_dinput.at[indices].add(out_grad_flat)\n",
    "\n",
    "        return dL_dinput\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        if self.use_legacy_v1:\n",
    "            output = self._maxpool2d_forward_legacy_v1(self.pool_size, self.stride, x)\n",
    "            self.output = output\n",
    "            self.max_indices = None  # Legacy doesn’t cache indices\n",
    "        else:\n",
    "            output, max_indices = self._maxpool2d_forward(self.pool_size, self.stride, x)\n",
    "            self.output = output\n",
    "            self.max_indices = max_indices\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "        if self.use_legacy_v1:\n",
    "            dL_dinput = self._maxpool2d_backward_legacy_v1(self.pool_size, self.input, output_grad, self.stride)\n",
    "        else:\n",
    "            dL_dinput = self._maxpool2d_backward(self.pool_size, self.stride, self.input, output_grad, self.max_indices)\n",
    "        self.grad_cache = {'dL_input': dL_dinput}  # Assuming ComputationNode expects this\n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2,3,30,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv2D(input_channels=3,kernel_size = 3, no_of_filters = 50, stride = 1, pad = 0, accumulate_grad_norm = False, accumulate_params = False,seed_key = None, bias = True, initialization = \"None\")\n",
    "max_pool = MaxPool2d(2,1)\n",
    "flatten = Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 50, 28, 28)\n",
      "(2, 50, 27, 27)\n",
      "(2, 36450)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03697681427001953"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "out = conv.forward(x)\n",
    "out = max_pool.forward(out)\n",
    "out = flatten.forward(out)\n",
    "et = time.time()\n",
    "et-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = np.random.randn(*list(out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 50, 27, 27)\n",
      "(Traced<ShapedArray(int32[72900])>with<DynamicJaxprTrace>, Traced<ShapedArray(int32[72900])>with<DynamicJaxprTrace>, Traced<ShapedArray(int32[72900])>with<DynamicJaxprTrace>, Traced<ShapedArray(int32[72900])>with<DynamicJaxprTrace>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15452051162719727"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "grad1 = flatten.backward(grad)\n",
    "grad2 = max_pool.backward(grad1)\n",
    "in_grad1= conv.backward(grad2)\n",
    "et = time.time()\n",
    "et-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MiniTorch.nets.base import Net\n",
    "from MiniTorch.nets.layers import Conv2D, Linear, MaxPool2d, PReLU, ReLU, SoftMax\n",
    "from MiniTorch.optimizers import SGD\n",
    "from MiniTorch.losses import CCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformers-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
