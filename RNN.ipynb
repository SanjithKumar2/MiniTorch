{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from MiniTorch.core.baseclasses import ComputationNode\n",
    "from MiniTorch.nets.layers import Linear, SoftMax, Tanh\n",
    "from MiniTorch.nets.base import Parameter, Net\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from MiniTorch.losses import CCE\n",
    "from MiniTorch.optimizers import AdaGrad\n",
    "from MiniTorch.nets.layers import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(ComputationNode):\n",
    "\n",
    "    def __init__(self, hidden_size, embed_dim,seq_len, accumulate_grad_norm=False, accumulate_params=False, initialization=\"xavier\"):\n",
    "        super().__init__()\n",
    "        self.h_size = hidden_size\n",
    "        self.emb_size = embed_dim\n",
    "        self.accumulate_grad_norm = accumulate_grad_norm\n",
    "        self.accumulate_params = accumulate_params\n",
    "        self.ini = initialization\n",
    "        self.parameters = {\n",
    "            'Wx' : None,\n",
    "            'Wh' : None,\n",
    "            'Wy' : None,\n",
    "            'bh' : None,\n",
    "            'by' : None\n",
    "        }\n",
    "        self.tanh = Tanh()\n",
    "\n",
    "    def initialize(self, seed_key):\n",
    "        import jax.random as jrandom\n",
    "        k1, k2, k3 = jrandom.split(seed_key, 3)\n",
    "        self.parameters['Wx'] = Parameter((self.emb_size,self.h_size),self.ini,k1)\n",
    "        self.parameters['Wh'] = Parameter((self.h_size, self.h_size),self.ini,k2)\n",
    "        self.parameters['Wy'] = Parameter((self.h_size, self.emb_size),self.ini,k3)\n",
    "        self.parameters['bh'] = Parameter((1, self.h_size),initialization=None, seed_key=None,is_bias=True)\n",
    "        self.parameters['by'] = Parameter((1, self.emb_size), initialization=None, seed_key=None,is_bias=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _rnn_forward(X, H_prev, Wx, Wh, Wy, bh, by,tanh):\n",
    "        H_next = (X @ Wx) + (H_prev @ Wh) + bh\n",
    "        H_next = tanh.forward(H_next)\n",
    "        out = H_next @ Wy + by\n",
    "        return H_next, out\n",
    "    \n",
    "    def forward(self, X, inference=False):\n",
    "        self.batch_size, self.seq_len, emb_size = X.shape\n",
    "        self.h_states = [jnp.zeros((self.batch_size,self.h_size))]\n",
    "        self.inp_states = []\n",
    "        self.out_states = []\n",
    "        X = jnp.transpose(X, (1, 0, 2))\n",
    "        for i in range(seq_len):\n",
    "            x = X[i,:,:]\n",
    "            self.inp_states.append(x)\n",
    "            H_next, out = self._rnn_forward(x, self.h_states[-1],self.parameters['Wx'].param,self.parameters['Wh'].param,self.parameters['Wy'].param,self.parameters['bh'].param,self.parameters['by'].param,self.tanh)\n",
    "            self.h_states.append(H_next)\n",
    "            self.out_states.append(out)\n",
    "        out_states = jnp.transpose(jnp.array(self.out_states),(1,0,2))\n",
    "        if inference:\n",
    "            return out_states[:,-1,:]\n",
    "        return out_states.reshape(self.batch_size*self.seq_len, emb_size)\n",
    "    def __call__(self, inference=False):\n",
    "        pass\n",
    "    def backward(self, out_grad):\n",
    "        out_grad = out_grad.reshape(self.batch_size, self.seq_len, self.emb_size)\n",
    "        out_grad = jnp.transpose(out_grad,(1, 0, 2))\n",
    "        dh_next = jnp.zeros_like(self.h_states[-1])\n",
    "        Wx, Wh, Wy, bh, by = self.parameters['Wx'].param,self.parameters['Wh'].param,self.parameters['Wy'].param,self.parameters['bh'].param,self.parameters['by'].param\n",
    "        self.parameters['Wx'].grad = jnp.zeros_like(Wx)\n",
    "        self.parameters['Wh'].grad = jnp.zeros_like(Wh)\n",
    "        self.parameters['Wy'].grad = jnp.zeros_like(Wy)\n",
    "        self.parameters['bh'].grad = jnp.zeros_like(bh)\n",
    "        self.parameters['by'].grad = jnp.zeros_like(by)\n",
    "        self.dL_dinput = []\n",
    "        for t in reversed(range(out_grad.shape[0])):\n",
    "            self.parameters['Wy'].grad += self.h_states[t].T @ out_grad[t]\n",
    "            self.parameters['by'].grad += jnp.sum(out_grad[t], axis=0)\n",
    "            dht = out_grad[t] @ Wy + dh_next\n",
    "            dth = self.tanh.backward(dht)\n",
    "            self.parameters['Wx'].grad += self.inp_states[t].T @ dth\n",
    "            self.parameters['Wh'].grad += self.h_states[t-1].T @ dth\n",
    "            self.parameters['bh'].grad += jnp.sum(dth,axis=0)\n",
    "            dh_next = dth @ Wh\n",
    "            dinput = dth @ Wx\n",
    "            self.dL_dinput.append(dinput)\n",
    "        return jnp.array(reversed(self.dL_dinput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    # Common words\n",
    "    \"hello\", \"world\", \"jax\", \"rocks\", \"gradient\", \"optimizer\", \"learning\", \"neural\", \"network\", \n",
    "    \"python\", \"model\", \"forward\", \"backward\", \"activation\", \"function\", \"training\", \"epoch\", \n",
    "    \"tensor\", \"loss\", \"update\", \"bias\", \"weight\", \"batch\", \"input\", \"output\", \"sequence\", \n",
    "    \"recurrent\", \"long\", \"short\", \"memory\", \"unit\", \"layer\", \"hidden\", \"state\", \"compute\",\n",
    "    # Names\n",
    "    \"alice\", \"bob\", \"charlie\", \"david\", \"eve\", \"frank\", \"grace\", \"heidi\", \"ivan\", \"judy\",\n",
    "    \"mallory\", \"oscar\", \"peggy\", \"trent\", \"victor\", \"wendy\",\n",
    "    # Technical extras\n",
    "    \"sigmoid\", \"tanh\", \"relu\", \"dropout\", \"regularization\", \"epoch\", \"loss\", \"accuracy\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 28\n",
      "PAD index: 26, EOS index: 27\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [\"<PAD>\", \"<EOS>\"]\n",
    "chars = sorted(list(set(\"\".join(corpus)))) + special_tokens\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "PAD_IDX = char2idx[\"<PAD>\"]\n",
    "EOS_IDX = char2idx[\"<EOS>\"]\n",
    "\n",
    "print(f\"Vocab Size: {vocab_size}\")\n",
    "print(f\"PAD index: {PAD_IDX}, EOS index: {EOS_IDX}\")\n",
    "def encode_sequence(seq):\n",
    "    \"\"\"Encodes a string + appends EOS\"\"\"\n",
    "    return [char2idx[ch] for ch in seq] + [EOS_IDX]\n",
    "\n",
    "def decode_sequence(indices):\n",
    "    \"\"\"Decodes indices, ignoring PAD and EOS tokens\"\"\"\n",
    "    chars_out = [idx2char[int(i)] for i in indices if idx2char[int(i)] not in (\"<PAD>\", \"<EOS>\")]\n",
    "    return \"\".join(chars_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(corpus):\n",
    "    X_data, y_data = [], []\n",
    "    for seq in corpus:\n",
    "        encoded = encode_sequence(seq)\n",
    "        X_data.append(encoded[:-1])  # input\n",
    "        y_data.append(encoded[1:])   # shifted output\n",
    "    return X_data, y_data\n",
    "\n",
    "def pad_sequences(seqs, pad_value=PAD_IDX):\n",
    "    max_len = max(len(s) for s in seqs)\n",
    "    return jnp.array([s + [pad_value]*(max_len - len(s)) for s in seqs])\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    return jnp.eye(num_classes)[y]\n",
    "\n",
    "\n",
    "def batch_iterator(X_data, y_data, batch_size=8):\n",
    "    n = len(X_data)\n",
    "    for i in range(0, n, batch_size):\n",
    "        X_batch = X_data[i:i+batch_size]\n",
    "        y_batch = y_data[i:i+batch_size]\n",
    "        X_batch = pad_sequences(X_batch)\n",
    "        y_batch = pad_sequences(y_batch)\n",
    "        y_one_hot = one_hot_encode(y_batch, len(chars))\n",
    "        X_batch_oh = one_hot_encode(X_batch, len(chars))\n",
    "        yield X_batch_oh, y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_output(output, idx_to_char, pad_idx=None, apply_softmax=True):\n",
    "    \"\"\"\n",
    "    Decodes one-hot or softmax outputs from the RNN into readable strings.\n",
    "\n",
    "    Args:\n",
    "        output: jnp.ndarray of shape (batch, seq_len, vocab_size)\n",
    "                — may be raw logits or already probabilities\n",
    "        idx_to_char: dict mapping integer index → character\n",
    "        pad_idx: index of the <PAD> token (optional)\n",
    "        apply_softmax: whether to apply softmax before decoding\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Decoded strings for each batch\n",
    "    \"\"\"\n",
    "    if apply_softmax:\n",
    "        # numerically stable softmax\n",
    "        exp_x = jnp.exp(output - jnp.max(output, axis=-1, keepdims=True))\n",
    "        output = exp_x / jnp.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    # Get the most likely token index at each timestep\n",
    "    token_indices = jnp.argmax(output, axis=-1)  # shape: (batch, seq_len)\n",
    "\n",
    "    decoded_strings = []\n",
    "    for seq in token_indices:\n",
    "        chars = []\n",
    "        for idx in seq:\n",
    "            if pad_idx is not None and idx == pad_idx:\n",
    "                continue  # skip padding\n",
    "            chars.append(idx_to_char[int(idx)])\n",
    "        decoded_strings.append(\"\".join(chars))\n",
    "\n",
    "    return decoded_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Net([RNN(\n",
    "    100,\n",
    "    28,\n",
    "    4\n",
    ")], 22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cce = CCE()\n",
    "optimizer = AdaGrad(0.01, model)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss->8.52025032043457\n",
      "Epoch 1: loss->8.138164520263672\n",
      "Epoch 2: loss->8.122602462768555\n",
      "Epoch 3: loss->8.245694160461426\n",
      "Epoch 4: loss->8.468669891357422\n",
      "Epoch 5: loss->8.71008014678955\n",
      "Epoch 6: loss->8.975908279418945\n",
      "Epoch 7: loss->9.315447807312012\n",
      "Epoch 8: loss->9.673639297485352\n",
      "Epoch 9: loss->10.086681365966797\n"
     ]
    }
   ],
   "source": [
    "X_data, y_data = prepare_sequences(corpus)\n",
    "for epoch in range(epochs):\n",
    "    ep_loss = 0\n",
    "    for X_batch, y_batch in batch_iterator(X_data, y_data, batch_size=4):\n",
    "        if X_batch.shape[0] != 4:\n",
    "            break\n",
    "        outputs = model.forward(X_batch)\n",
    "        b, sq, dim = y_batch.shape\n",
    "        y_batch = y_batch.reshape(b*sq,dim)\n",
    "        loss = cce.loss(outputs, y_batch)\n",
    "        ini_grad = cce.backward()\n",
    "        optimizer.step(ini_grad)\n",
    "        ep_loss += loss\n",
    "    print(f\"Epoch {epoch}: loss->{ep_loss/4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.reshape(4*5,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.tanh(0) * (1/(1+jnp.exp(-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_backward(Wb, R_out, X, d, eps=0.001, bias_factor=0.0):\n",
    "        n_layers = len(Wb)//4\n",
    "        b, T, _ = X.shape\n",
    "        device = X.device\n",
    "        X = X.permute(1,0,2)\n",
    "        \n",
    "        Rgates = torch.zeros((n_layers, T, b, d)).to(device) #following the candidate gate take all method supported by Deep Taylor Decomposition.\n",
    "        Rh_states = torch.zeros((n_layers, T+1, b, d)).to(device)\n",
    "        Rc_states = torch.zeros((n_layers, T+1, b, d)).to(device)\n",
    "        Rh_states[-1, T-1] = R_out\n",
    "        Rx = torch.zeros((T, b, d))\n",
    "        # format reminder: lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor)\n",
    "        for n in reversed(range(n_layers)):\n",
    "            Wih, Whh, bih, bhh = Wb[f\"weight_ih_l{n}\"].T, Wb[f\"weight_hh_l{n}\"].T, Wb[f\"bias_ih_l{n}\"].T, Wb[f\"bias_hh_l{n}\"].T\n",
    "            idx_no_gx = torch.concat([torch.arange(0, d), torch.arange(d, 2*d), torch.arange(d*3, d*4)]) # indices for everything except mem (candidate) gate\n",
    "            ix, fx, gx, ox = torch.arange(0, d), torch.arange(d, 2*d), torch.arange(d*2, d*3), torch.arange(d*3, d*4) # indices for inp, forget, candidate and out gates\n",
    "            \n",
    "            for t in reversed(range(T)):\n",
    "                Rc_states[n, t] += Rh_states[n, t]\n",
    "                Rc_states[n, t-1] = lrp_linear(gates[n, t, :, fx] * c_states[n, t-1], torch.eye(d).to(device), torch.zeros((d)).to(device), c_states[n, t], Rc_states[n, t],d,eps,bias_factor)\n",
    "                Rgates[n, t] = lrp_linear(gates[n, t, :, gx]*gates[n, t, :, ix], torch.eye(d).to(device), torch.zeros((d)).to(device), c_states[n, t], Rc_states[n, t],d,eps,bias_factor)\n",
    "                if n == 0:\n",
    "                    x = X[t]\n",
    "                    Rx[t] = lrp_linear(x,Wih[gx],bih[gx]+bhh[gx],pre_act[n, t, :, gx], Rgates[n, t],d+d,eps,bias_factor)\n",
    "                else:\n",
    "                    x = h_states[n-1, t]\n",
    "                    Rh_states[n-1, t] = lrp_linear(x,Wih[gx],bih[gx]+bhh[gx],pre_act[n, t, :, gx], Rgates[n, t],d+d,eps,bias_factor)\n",
    "                Rh_states[n, t-1] = lrp_linear(h_states[n, t-1],Whh[gx],bih[gx]+bhh[gx],pre_act[n, t, :, gx], Rgates[n, t],d+d,eps,bias_factor)\n",
    "                \n",
    "        return Rx.permute(1, 0, 2)\n",
    "                    \n",
    "                \n",
    "                \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xformers-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
